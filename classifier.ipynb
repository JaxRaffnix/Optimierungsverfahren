{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c61421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from itertools import product\n",
    "import copy\n",
    "import math\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e94c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.float32\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62ae4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = 12\n",
    "# MAX_ROUNDS = 3\n",
    "NUM_TRIALS = 20\n",
    "\n",
    "iters_per_epoch = np.ceil(NUM_TRAIN / BATCH_SIZE)\n",
    "\n",
    "RANDOM_SEED = 0\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27950d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = T.Compose([\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "transform_val_test = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "cifar10_train = dset.CIFAR10('datasets', train=True, download=True, transform=transform_train)\n",
    "cifar10_val_test = dset.CIFAR10('datasets', train=True, download=True, transform=transform_val_test)\n",
    "cifar10_test = dset.CIFAR10('datasets', train=False, download=True, transform=transform_val_test)\n",
    "\n",
    "indices = np.arange(len(cifar10_train))\n",
    "np.random.shuffle(indices)\n",
    "train_idx = indices[:NUM_TRAIN]\n",
    "val_idx = indices[NUM_TRAIN:]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    cifar10_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "loader_val = DataLoader(\n",
    "    cifar10_val_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=val_sampler,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(loader_train)}, Val batches: {len(loader_val)}\")\n",
    "print(f\"Results in {iters_per_epoch} iterations per epoch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e36b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    num_epochs=10,\n",
    "    criterion=None,\n",
    "    device=None,\n",
    "    print_every=20,\n",
    "    patience=3,\n",
    "    accumulation_steps=1,\n",
    "    scheduler=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimized training function with AMP, early stopping, gradient accumulation, \n",
    "    best model saving, and optional scheduler.\n",
    "    \"\"\"\n",
    "    device = device or torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    criterion = criterion or F.cross_entropy\n",
    "    optimizer = optimizer\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_acc = -float('inf')\n",
    "    best_model = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    train_accs, val_accs, lrs, epoch_times = [], [], [], []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad() if accumulation_steps == 1 else None\n",
    "\n",
    "            with autocast(\"cuda\"):\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y) / accumulation_steps  # scale loss for accumulation\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item() * accumulation_steps\n",
    "            if (i + 1) % print_every == 0:\n",
    "                iter_time = (time.time() - start_epoch_time) / (i + 1)\n",
    "                iters_per_sec = 1 / iter_time\n",
    "                print(f\"Epoch {epoch+1}, Iter {i+1}, Avg Loss: {running_loss/print_every:.4f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Step scheduler at epoch end\n",
    "        if scheduler is not None:\n",
    "            lrs.append(scheduler.get_last_lr()[0])\n",
    "            scheduler.step()\n",
    "\n",
    "        # Validation check\n",
    "        val_acc = get_accuracy(val_loader, model, f\"Validation Epoch {epoch+1}\", device)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        train_acc = get_accuracy(train_loader, model, f\"Training Epoch {epoch+1}\", device=device)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Early stopping\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model = copy.deepcopy(model)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                break\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "\n",
    "    return {\n",
    "        \"model\": best_model if best_model else model,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"lrs\": lrs,\n",
    "        \"epoch_times\": epoch_times,\n",
    "        \"total_time\": total_time\n",
    "    }\n",
    "\n",
    "\n",
    "def get_accuracy(loader, model, name=\"Validation\", device=device, debug=True):\n",
    "    \"\"\"\n",
    "    Compute accuracy of the model on a given DataLoader.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with autocast(\"cuda\"):\n",
    "                scores = model(x)\n",
    "            preds = scores.argmax(dim=1)\n",
    "            correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    acc = correct / total\n",
    "    if debug:\n",
    "        print(f\"{name} Accuracy: {100*acc:.2f}%\")\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a7527",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = None\n",
    "best_val_acc = -float('inf')\n",
    "\n",
    "def objective(trial):\n",
    "    global best_model, best_val_acc\n",
    "\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    n_epochs = trial.suggest_int(\"n_epochs\", 5, 20)       \n",
    "    \n",
    "    # Build model\n",
    "    ch1 = 16\n",
    "    ch2 = 64\n",
    "    ch3 = 32\n",
    "    weight_decay = 2.075612963823467e-06\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(3, ch1, 5, padding=2),\n",
    "        nn.BatchNorm2d(ch1),\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(ch1, ch2, 3, padding=1),\n",
    "        nn.BatchNorm2d(ch2),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "\n",
    "        nn.Conv2d(ch2, ch3, 3, padding=1),\n",
    "        nn.BatchNorm2d(ch3),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2,2),\n",
    "\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(ch3 * 8 * 8, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.25),\n",
    "        nn.Linear(128, 10)\n",
    "    ).to(device)\n",
    "            \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs)\n",
    "\n",
    "    training_result = train_model(\n",
    "        model=model,\n",
    "        train_loader=loader_train,\n",
    "        val_loader=loader_val,\n",
    "        num_epochs=n_epochs,\n",
    "        device=device,\n",
    "        print_every=len(loader_train) // 4,\n",
    "        patience=2,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    "    trained_model = training_result[\"model\"]\n",
    "    \n",
    "    val_acc = get_accuracy(loader_val, trained_model, device=device, debug=False)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model = copy.deepcopy(trained_model)\n",
    "        print(f\"âœ… Found new best model: {100*val_acc:.2f}\")\n",
    "\n",
    "    results.append({\n",
    "        \"trial\": trial.number,\n",
    "        \"ch1\": ch1, \"ch2\": ch2, \"ch3\": ch3,\n",
    "        \"lr\": lr, \"weight_decay\": weight_decay,\n",
    "        \"val_acc\": val_acc, \n",
    "        \"train_accs\": training_result[\"train_accs\"],\n",
    "        \"val_accs\": training_result[\"val_accs\"],\n",
    "        \"lrs\": training_result[\"lrs\"],\n",
    "        \"runtime\": training_result[\"total_time\"]\n",
    "    })\n",
    "    \n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198aa1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_name = \"bayesian_optimization\"\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "\n",
    "summaries = optuna.study.get_all_study_summaries(storage=storage_name)\n",
    "existing_studies = [s.study_name for s in summaries]\n",
    "\n",
    "if study_name in existing_studies:\n",
    "    optuna.delete_study(study_name=study_name, storage=storage_name)\n",
    "    print(f\"Deleted existing study: {study_name}\")\n",
    "\n",
    "# Create or load the study\n",
    "study = optuna.create_study(\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    direction=\"maximize\",\n",
    "    # load_if_exists=True\n",
    ")\n",
    "\n",
    "# Run optimization\n",
    "study.optimize(objective, n_trials=NUM_TRIALS)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "print(\"Best hyperparameters:\", best_trial.params)\n",
    "print(f\"Best validation accuracy: {100*best_trial.value:.2f}%\")\n",
    "\n",
    "torch.save(best_model, \"best model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bce983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add random search"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
